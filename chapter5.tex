\chapter{Conclusions}

\label{sec:conclusions}

In this study, we present Fair Transition Loss, a novel in-processing technique for addressing fair classification problems. It leverages concepts from label noise robustness to mitigate social bias against underprivileged groups. We delve into the intersection of these two research areas, highlighting both their similarities and differences. Our approach tackles the fairness-performance trade-off as a multi-objective optimization problem, employing a linear relaxed objective function to reduce bias while maintaining acceptable predictive performance levels. We benchmark this approach and compare to proeminent in-processing techniques in common fair classification tasks, using the Almost Stochastic Order test to evaluate results through multiple resampling iterations. This ensures that all methods operate under the same conditions, maximizing their potential within the scope of hyperparameter tuning.

This is the first technique that models fair classification problems by drawing insights from classification in the presence of label noise. Our experiments indicate that Fair Transition Loss consistently outperforms its competitors in most optimization scenarios. Even in those cases that the proposed method isn't the outright leader, it performs at least as well as evaluated alternatives. Therefore, this novel approach can significantly mitigate bias while keeping model performance, particularly in scenarios optimizing balanced performance metrics like MCC. The proposed technique particularly stands out in setups where hyperparameter tuning is an integral component of the prediction pipeline.

While our proposed method seems competitive in problems involving hyperparameter optimization for binary fair classification tasks using a simple Multi-Layer Perceptron, we can outline some potential research directions: evaluate Fair Transition Loss within different neural network architectures, such as Deep Neural Networks; investigate whether the proposed method can effectively address multi-class fair classification problems and handle multiple sensitive attributes, as theoretically possible; evaluate FTL within different multi objective optimization schemes, such as the Fair Hyperparameter Tuning techniques proposed by~\cite{Cruz2021} or the non-linear Chebyshev scalarization scheme proposed by~\cite{Wei2022}; explore approaches to estimating or initializing transition matrices without relying on hyperparameter tuning techniques. 

With this work, we hope to establish Fair Transition Loss as a valuable tool in fair classification tasks and pave the way for novel approaches that draw insights from label noise for various fair machine learning problems, including regression, recommender systems, ranking and language models.

\section{Considerations on the proposal}

\section{Results summary}

colocar uma tabela pequena para FTL e outra para regularização aqui

\section{Contributions}

\section{Research directions}