\chapter{Introduction}

Brisha Borden was running late to pick up her god-sister from school. She and her friend, both 18-year-old girls, took an unlocked kidâ€™s bicycle and scooter and ride for a while. They were arrested and charged with theft. Meanwhile, Vernon Prater, a seasoned criminal with prior history of armed robbery and years in prison, was caught by theft tools. A computer program predicted their future criminal behavior: Borden, a black woman, was rated high risk; Prater, a white man, was rated low risk. However, the algorithm got it wrong. Borden remained crime-free, while Prater was again serving an eight-year prison term for a subsequent theft.

This famous case revealed by a ProPublica's investigative reporting evidenced the biased behavior of Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), a criminal risk assessment tool owned by Northpointe and used by Florida's criminal justice system~\citep{machine_bias}. Examples like that are not extraordinary, decision making systems are incurring in bias by reproducing prejudices and disparities from society all around the world. Although the machine learning bias can not be resolved only by computing science and statistics solutions, we delve into this issue in order to provide tools for decision makers, civil society and other stakeholders to tackle this humongous problem. 

\section{Contextualization}

The issue of fairness in machine learning has recently risen to prominence due to its implications in real-world decision-making systems~\citep{Mehrabi2019,Hutchinson2019}. Addressing biases and discrimination is a relevant frontier in decision-making systems, as equitable outcomes across various demographic groups is both an ethical imperative and often a legal requirement. Though fairness is a multifaceted concept, it has been deeply examined within the context of machine learning. The literature presents a variety of fairness definitions, drawing concepts from political philosophy and computational techniques~\citep{caton2023, Hutchinson2019}. Choosing an equitable machine learning model requires the selection of a fitting definition of fairness, tailored to the specific problem at hand. Many such definitions can be precisely articulated, allowing models to be evaluated based on their predictions.

One inherent challenge in fair machine learning is the balance between fairness and predictive performance. Efforts to mitigate unfairness often compromise the model's predictive performance, a trade-off that has been well documented~\citep{Mehrabi2019, caton2023}. Predictors that are less biased against marginalized groups may deviate from the true class, resulting in sub-optimal performance. Also, introducing fairness considerations adds constraints to the model, further complicating the optimization process~\citep{Zafar2017b}. 

In this context sounds intuitive the idea of just removing protected attributes such as race, sex and age, so that the model would be unable to produce outcomes using these information in a biased way. Unfortunately this approach not only does not work, as the model will probably infer these characteristics using proxy features available on data, but also it can accentuate disparities due lack of information. This tricky phenomena is know as Redlining Effect~~\citep{Pedreschi2008} and was extensively documented~~\citep{Mehrabi2019,caton2023,Hort2023}.

\section{Objectives}

Although noise and unfairness in the context of classification are not identical phenomena, there are relevant similarities, especially in their effects on data. Both changes the appropriate or expected values of an instance's attributes or label. While noise corrupts the observable value of an attribute or class without affecting the true value, unfairness acts directly on the social reality that generates these values, corrupting both the observable and true values. Considering the succeeds approaches to produce label noise robustness through loss correction~\citep{Patrini2017}, which uses different levels of uncertainty according observed label, we raise the following hypothesis:

\begin{hypothesis}\label{hyp:ftl}
A loss correction approach that considers different levels of corruption according individual's sensitive attribute and available class reduces unfairness. 
\end{hypothesis}

Thus, the main objective of this study is to produce a loss correction tailored to reconsider the predicted outcomes of a binary classifier taking into account different levels of unfairness according individual's social group and available class, addressing Hypothesis~\ref{hyp:ftl}.

Another relevant topic tackled by this work is the Redlining Effect, which perversely penalizes individuals of socially marginalized groups, even on well-intentioned approaches to mitigate unfairness. Intuitively, reducing model's reliance on those sensitive feature's proxies would promote fairness. Thus, in order to achieve a Redlining Effect robustness, we raise the following hypothesis:

\begin{hypothesis}\label{hyp:rpr}
A regularization approach that penalizes model's dependency on sensitive attribute proxies reduces the effects of redlining an thus promoting fairness. 
\end{hypothesis}

Therefore, to accomplish Hypothesis~\ref{hyp:rpr} this study pursue this secondary objective of developing a regularization approach capable of properly identifying and penalizing the proxies of sensitive feature. 

\section{Contributions}

In light of referred challenges, we introduce the Fair Transition Loss, a novel approach to fair classification. This method estimates the influence of historical and societal biases on outcome probabilities for distinct groups within dataset. For instance, individuals from marginalized groups might have lower chances of favorable outcomes compared to their counterparts from privileged groups. Such disparate probabilities can be represented by transition matrices. Drawing inspiration from label noise robustness, we incorporate these transition matrices information into the loss function to promote fairness. The proposed method has some hyperarameters, chosen by a Multi-Objective Optimization approach combining both fairness and model performance with a linear objective. This objective function is defined in such a way that the proposed approach is suitable to optimize a wide range of fairness and performance metrics.

The primary contribution of this study is the conceptualization of the Fair Transition Loss, a novel loss function influenced by label noise methodologies. The novelty of this work lies in applying label noise techniques directly within the model to mitigate unfairness. As far as we know, this is the first work to adopt label noise techniques directly to address fairness in machine learning. Additionally, the proposed method achieves state-of-art results in benchmark tests across common fair classification tasks. Our empirical results demonstrate that this method consistently outperforms many leading fair classification techniques in a variety of scenarios. This conceptualization and results are also published at \cite{Canalli2024}.

Although we draw some comparison between unfairness and noise concepts, delineating similarities and differences, it is important to emphasizes that this works does not describes bias and discrimination issues as a noise phenomena. Instead, we use some transition-matrix-based robust loss function from label noise literature in order to create a loss correction approach suitable to fair classification problems. The core concept here is to adjust the individual's probabilities according social group (i.e. protected and privileged) achieving the favorable or unfavorable outcome on a binary classification problem, which is attained trough a custom loss function considering the unfairness information from data in a transition matrix format.

Another contribution of the present study is a novel regularization approach to fair classification named Redlining Penalty Regularization, which uses feature's Chatterjee's xi correlation~~\citep{chatterjee2020new}  to the sensitive attribute in order to proportionately penalize model's dependency on it. Our empirical evaluation demonstrate that this approach effectively mitigate unfairness while keeping predictive performance, with benefits corresponding to redlining level on dataset. Also, we demonstrate that this approach can reduce bias while keeping predictive performance on a variety of scenarios, enhancing performance-fairness trade-off on both standard neural networks and those trained with Fair Transition Loss. To the best of our knowledge this is the first regularization approach to penalize model's dependency proportionately to feature's correlation to protected attribute in order to mitigate unfairness. 

\section{Results summary}

Our empirical evaluation compares the proposed methods with its counterparts on the fair classification benchmark datasets \textit{Adult Income}~\citep{misc_adult_2}, \textit{Bank Market}~\citep{misc_bank_marketing_222}, \textit{COMPAS Recidivism}~\citep{misc_compas}, and \textit{German Credit}~\citep{misc_statlog_(german_credit_data)_144} under multiple optimization objectives, each of them targeting maximize a predictive performance metric while minimizing a fairness metric. In every comparison we optimize all methods through an extensive hyperparameter tuning process, in order to guarantee that each one has the same competitive conditions. After the hyperparameter tuning phase the method is than retrained with the best hyperparameters and evaluated using a test set not used on tuning, whose metrics are reported in results. This complete process is then repeated over multiple runs through dataset resampling and an Almost Stochastic Order~\citep{dror2019deep} comparison is performed, which is a significance test suitable to compare complex machine learning models under hyperparameter optimization. The comparative studies are conducted under $4$ datasets to $6$ optimization objectives, which lead us to $24$ evaluation scenarios each. A complete description of these experimental evaluations are available at Section~\ref{sec:ftl_experimental} and Section~\ref{sec:rpr_experimental}, while the computational implementations, results and analysis are available at \cite{canalli2024_zenodo} as supplementary material.

In order to properly attend to Hypothesis~\ref{hyp:ftl} we compare the proposed loss correction named Fair Transition Loss (Section~\ref{sec:ftl_proposal}) with a standard Multi-Layer Perceptron and with classical and state-of-art fair classification approaches. The experimental results available at Section~\ref{sec:ftl_results} can be summarized as follow:
\begin{itemize}
    \item The proposed model achieves the best result on most of evaluated scenarios;
    \item The proposed model is the only one with competitive results in all evaluated scenarios.
\end{itemize}

Also, to attend to Hypothesis~\ref{hyp:rpr} we compare the proposed regularization approach named Redlining Penalty Regularization (Section~\ref{sec:rpr_proposal}) applied on a standard Multi-Layer Perceptron and on a Multi-Layer Perceptron with Fair Transition Loss. The experimental results available at Section~\ref{sec:rpr_results} can be summarized as follow:
\begin{itemize}
    \item The proposed regularization approach enhances the results on most of evaluated scenarios;
    \item The proposed regularization approach performs according the redlining level of the dataset, the higher the redlining effect higher the improvement.
\end{itemize}

\section{Thesis structure}

The remainder of this Thesis is structured as follows: Chapter~\ref{chap:fairness} delves into Fair Machine Learning describing core concepts, some related research topics, sources and types of algorithmic unfairness, definitions, metrics and fair classification approaches; Chapter~\ref{chap:ftl}, describes and evaluates Fair Transition Loss and its underlying principles, delineating related works encompassing multi-objective optimization in fair machine learning, classifications in the presence of label noise and studies bridging fairness and label noise. Chapter~\ref{chap:rpr}, describes and evaluates Redlining Penalty Regularizer and its fundamentals, describing some correlation coefficients suitable to this use and related works that applies a regularization approach to promote fairness; Chapter~\ref{chap:conclusions} presents conclusions drawn from our study, fashioning some considerations on the proposed methods, delineating the contributions and some insights to further research.