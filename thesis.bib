@InProceedings{durillo2009,
author="Durillo, Juan J.
and Nebro, Antonio J.
and Luna, Francisco
and Alba, Enrique",
editor="Ehrgott, Matthias
and Fonseca, Carlos M.
and Gandibleux, Xavier
and Hao, Jin-Kao
and Sevaux, Marc",
title="On the Effect of the Steady-State Selection Scheme in Multi-Objective Genetic Algorithms",
booktitle="Evolutionary Multi-Criterion Optimization",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="183--197",
abstract="Genetic Algorithms (GAs) have been widely used in single-objective as well as in multi-objective optimization for solving complex optimization problems. Two different models of GAs can be considered according to their selection scheme: generational and steady-state. Although most of the state-of-the-art multi-objective GAs (MOGAs) use a generational scheme, in the last few years many proposals using a steady-state scheme have been developed. However, the influence of using those selection strategies in MOGAs has not been studied in detail. In this paper we deal with this gap. We have implemented steady-state versions of the NSGA-II and SPEA2 algorithms, and we have compared them to the generational ones according to three criteria: the quality of the resulting approximation sets to the Pareto front, the convergence speed of the algorithm, and the computing time. The results show that multi-objective GAs can profit from the steady-state model in many scenarios.",
isbn="978-3-642-01020-0"
}

@inproceedings{HuXT23,
  author       = {Zhihao Hu and
                  Yiran Xu and
                  Xinmei Tian},
  title        = {Adaptive Priority Reweighing for Generalizing Fairness Improvement},
  booktitle    = {International Joint Conference on Neural Networks, {IJCNN} 2023, Gold
                  Coast, Australia, June 18-23, 2023},
  pages        = {1--8},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/IJCNN54540.2023.10191757},
  doi          = {10.1109/IJCNN54540.2023.10191757},
  timestamp    = {Thu, 24 Aug 2023 09:27:55 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcnn/HuXT23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{WZhang2022,
  author       = {Wenbin Zhang and
                  Jeremy C. Weiss},
  title        = {Longitudinal Fairness with Censorship},
  booktitle    = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2022, Thirty-Fourth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2022, The Twelveth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2022 Virtual Event, February 22
                  - March 1, 2022},
  pages        = {12235--12243},
  publisher    = {{AAAI} Press},
  year         = {2022},
  url          = {https://doi.org/10.1609/aaai.v36i11.21484},
  doi          = {10.1609/AAAI.V36I11.21484},
  timestamp    = {Mon, 04 Sep 2023 16:50:21 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/0002W22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{WZhang2023_b,
  author       = {Wenbin Zhang and
                  Jeremy C. Weiss},
  title        = {Fairness with censorship and group constraints},
  journal      = {Knowl. Inf. Syst.},
  volume       = {65},
  number       = {6},
  pages        = {2571--2594},
  year         = {2023},
  url          = {https://doi.org/10.1007/s10115-023-01842-5},
  doi          = {10.1007/S10115-023-01842-5},
  timestamp    = {Sat, 29 Apr 2023 19:26:46 +0200},
  biburl       = {https://dblp.org/rec/journals/kais/ZhangW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{WZhang2023_a,
title={Censored Fairness through Awareness},
volume={37},
url={https://ojs.aaai.org/index.php/AAAI/article/view/26708},
DOI={10.1609/aaai.v37i12.26708},
abstractNote={There has been increasing concern within the machine learning community and beyond that Artificial Intelligence (AI) faces a bias and discrimination crisis which needs AI fairness with urgency. As many have begun to work on this problem, most existing work depends on the availability of class label for the given fairness definition and algorithm which may not align with real-world usage. In this work, we study an AI fairness problem that stems from the gap between the design of a &quot;fair&quot; model in the lab and its deployment in the real-world. Specifically, we consider defining and mitigating individual unfairness amidst censorship, where the availability of class label is not always guaranteed due to censorship, which is broadly applicable in a diversity of real-world socially sensitive applications. We show that our method is able to quantify and mitigate individual unfairness in the presence of censorship across three benchmark tasks, which provides the first known results on individual fairness guarantee in analysis of censored data.},
number={12},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Zhang, Wenbin and Hernandez-Boussard, Tina and Weiss, Jeremy},
year={2023},
month={Jun.},
pages={14611-14619}
}

@inproceedings{jiang2020identifying,
  title={Identifying and correcting label bias in machine learning},
  author={Jiang, Heinrich and Nachum, Ofir},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={702--712},
  year={2020},
  organization={PMLR}
}


@inproceedings{mroueh2021fair,
  title={Fair Mixup: Fairness via Interpolation},
  author={Mroueh, Youssef and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{roh2020fairbatch,
  title={Fairbatch: Batch selection for model fairness},
  author={Roh, Yuji and Lee, Kangwook and Whang, Steven Euijong and Suh, Changho},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{WZhang2023_c,
  author       = {Wenbin Zhang and
                  Juyong Kim and
                  Zichong Wang and
                  Pradeep Ravikumar and
                  Jeremy C. Weiss},
  title        = {Individual Fairness Guarantee in Learning with Censorship},
  journal      = {CoRR},
  volume       = {abs/2302.08015},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.08015},
  doi          = {10.48550/ARXIV.2302.08015},
  eprinttype    = {arXiv},
  eprint       = {2302.08015},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-08015.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{VucinichZ23,
  author       = {Sean Vucinich and
                  Qiang Zhu},
  title        = {The Current State and Challenges of Fairness in Federated Learning},
  journal      = {{IEEE} Access},
  volume       = {11},
  pages        = {80903--80914},
  year         = {2023},
  url          = {https://doi.org/10.1109/ACCESS.2023.3295412},
  doi          = {10.1109/ACCESS.2023.3295412},
  timestamp    = {Fri, 18 Aug 2023 08:45:20 +0200},
  biburl       = {https://dblp.org/rec/journals/access/VucinichZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ChenZZZY24,
  author       = {Huiqiang Chen and
                  Tianqing Zhu and
                  Tao Zhang and
                  Wanlei Zhou and
                  Philip S. Yu},
  title        = {Privacy and Fairness in Federated Learning: On the Perspective of
                  Tradeoff},
  journal      = {{ACM} Comput. Surv.},
  volume       = {56},
  number       = {2},
  pages        = {39:1--39:37},
  year         = {2024},
  url          = {https://doi.org/10.1145/3606017},
  doi          = {10.1145/3606017},
  timestamp    = {Fri, 27 Oct 2023 20:40:08 +0200},
  biburl       = {https://dblp.org/rec/journals/csur/ChenZZZY24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhang2018,
   abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
   author = {Brian Hu Zhang and Blake Lemoine and Margaret Mitchell},
   doi = {10.1145/3278721.3278779},
   isbn = {9781450360128},
   journal = {AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {adversarial learning,debiasing,multi-task learning,unbiasing},
   month = {12},
   pages = {335-340},
   publisher = {Association for Computing Machinery, Inc},
   title = {Mitigating Unwanted Biases with Adversarial Learning},
   year = {2018}
}


@misc{Li2018,
   abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
   author = {Lisha Li and Kevin Jamieson and Afshin Rostamizadeh and Ameet Talwalkar},
   journal = {Journal of Machine Learning Research},
   keywords = {deep learning,hyperparameter optimization,infinite-armed bandits,model selection,online optimization},
   pages = {1-52},
   title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
   volume = {18},
   url = {http://jmlr.org/papers/v18/16-558.html.},
   year = {2018},
}

@inproceedings{bergstra2011,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
volume = {24},
 year = {2011},
 url = {https://proceedings.neurips.cc/paper\_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
}

@article{Li2016,
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {6765–6816},
numpages = {52},
keywords = {hyperparameter optimization, deep learning, model selection, online optimization, infinite-armed bandits}
}


@InProceedings{Li2020,
  title = 	 {Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks},
  author =       {Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4313--4324},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/li20j/li20j.pdf},
  url = 	 {https://proceedings.mlr.press/v108/li20j.html},
  abstract = 	 {Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including significantly corrupted ones. Despite this (over)fitting capacity in this paper we demonstrate that such overparameterized networks have an intriguing robustness capability: they are surprisingly robust to label noise when first order methods with early stopping is used to train them. This paper also takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy labels network must stray rather far from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.}
}


@article{Li2019GradientDW,
  title={Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks},
  author={Mingchen Li and Mahdi Soltanolkotabi and Samet Oymak},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.11680},
  url={https://api.semanticscholar.org/CorpusID:85543143}
}

@book{Morales-Hernandez2023,
abstract = {Hyperparameter optimization (HPO) is a necessary step to ensure the best possible performance of Machine Learning (ML) algorithms. Several methods have been developed to perform HPO; most of these are focused on optimizing one performance measure (usually an error-based measure), and the literature on such single-objective HPO problems is vast. Recently, though, algorithms have appeared that focus on optimizing multiple conflicting objectives simultaneously. This article presents a systematic survey of the literature published between 2014 and 2020 on multi-objective HPO algorithms, distinguishing between metaheuristic-based algorithms, metamodel-based algorithms and approaches using a mixture of both. We also discuss the quality metrics used to compare multi-objective HPO procedures and present future research directions.},
archivePrefix = {arXiv},
arxivId = {2111.13755},
author = {Morales-Hern{\'{a}}ndez, Alejandro and {Van Nieuwenhuyse}, Inneke and {Rojas Gonzalez}, Sebastian},
booktitle = {Artificial Intelligence Review},
address = {New York, NY, USA},
doi = {10.1007/s10462-022-10359-2},
eprint = {2111.13755},
file = {:home/canalli/Downloads/s10462-022-10359-2.pdf:pdf},
isbn = {0123456789},
issn = {15737462},
keywords = {Hyperparameter optimization,Machine learning,Meta-heuristic,Metamodel,Multi-objective optimization},
mendeley-groups = {Fairness/multiobjective optimization},
number = {8},
pages = {8043--8093},
publisher = {Springer Netherlands},
title = {{A survey on multi-objective hyperparameter optimization algorithms for machine learning}},
url = {https://doi.org/10.1007/s10462-022-10359-2},
volume = {56},
year = {2023}
}




@article{Petrovic2021,
abstract = {Artificial intelligence is steadily increasing its impact on everyday life. Therefore, the societal issues of artificial intelligence have become an important concern in the AI research. The presence of data that reflects human biases towards historically discriminated groups defined by sensitive features such as racefile:///home/canalli/Downloads/1805.11202.pdf and gender, results in machine learning models which discriminate against these groups. In order to tackle the impact of bias in data, researchers developed a variety of specialized machine learning algorithms which are able to satisfy different fairness constraints imposed on the model. Group fairness constraints do not fit standard machine learning formulations easily due to their non-differentiable nature. In this paper we developed a technique for learning a fair classifier by Monte Carlo policy gradient method which naturally deals with such non-differentiable constraints. Our methodology focuses on direct optimization of both group fairness metric and predictive performance of the model. In addition, we propose two different variance reduction techniques of gradient estimation. We compare our models to seven other related and state-of-the-art models and demonstrate that they are able to achieve better trade-off between accuracy and unfairness. To the best of our knowledge, this is the first fair classification algorithm which solves the issue of non-differentiable constraints by reinforcement learning techniques.},
author = {Petrovi{\'{c}}, Andrija and Nikoli{\'{c}}, Mladen and Jovanovi{\'{c}}, Milo{\v{s}} and Bijani{\'{c}}, Milo{\v{s}} and Deliba{\v{s}}i{\'{c}}, Boris},
doi = {10.1016/j.engappai.2021.104398},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrovi{\'{c}} et al. - 2021 - Fair classification via Monte Carlo policy gradient method.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Combinatorial optimization,Deep learning,Fairness,REINFORCE,Reinforcement learning},
mendeley-groups = {Fairness/multiobjective optimization,Fairness},
number = {February},
pages = {104398},
publisher = {Elsevier Ltd},
title = {{Fair classification via Monte Carlo policy gradient method}},
url = {https://doi.org/10.1016/j.engappai.2021.104398},
volume = {104},
year = {2021}
}

@Inproceedings{Schmucker2020,
 author = {Robin Schmucker and Michele Donini and Valerio Perrone and Cédric Archambeau},
 title = {Multi-objective multi-fidelity hyperparameter optimization with application to fairness},
 year = {2020},
 booktitle = {NeurIPS 2020 Workshop on Meta-learning},
}

@article{Adel2019, 
title={One-Network Adversarial Fairness}, 
volume={33}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/4085}, 
DOI={10.1609/aaai.v33i01.33012412}, 
abstractNote={&lt;p&gt;There is currently a great expansion of the impact of machine learning algorithms on our lives, prompting the need for objectives other than pure performance, including fairness. Fairness here means that the outcome of an automated decisionmaking system should not discriminate between subgroups characterized by sensitive attributes such as gender or race. Given any existing differentiable classifier, we make only slight adjustments to the architecture including adding a new hidden layer, in order to enable the concurrent adversarial optimization for fairness and accuracy. Our framework provides one way to quantify the tradeoff between fairness and accuracy, while also leading to strong empirical performance.&lt;/p&gt;}, 
number={01}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Adel, Tameem and Valera, Isabel and Ghahramani, Zoubin and Weller, Adrian}, 
year={2019}, 
month={Jul.}, 
pages={2412-2420} 
}


@article{Komiyama2018,
abstract = {The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.},
author = {Komiyama, Junpei and Takeda, Akiko and Honda, Junya and Shimao, Hajime},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Komiyama et al. - 2018 - Nonconvex optimization for regression with fairness constraints.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
mendeley-groups = {Fairness/multiobjective optimization},
pages = {4280--4294},
title = {{Nonconvex optimization for regression with fairness constraints}},
volume = {6},
year = {2018}
}

@misc{misc_adult_2,
  author       = {Becker,Barry and Kohavi,Ronny},
  title        = {{Adult}},
  year         = {1996},
  howpublished = {UCI Machine Learning Repository},
  url          = {https://archive.ics.uci.edu/dataset/2/adult},
  doi          = {10.24432/C5XW20}
}

@misc{misc_compas,
  author       = {Jeff Larson, Surya Mattu, Lauren Kirchner and Julia Angwin},
  title        = {{COMPAS Dataset}},
  year         = {2016},
  howpublished = {ProPublica},
  url          = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}
}

@misc{misc_bank_marketing_222,
  author       = {S. Moro, P. Rita and P. Cortez},
  title        = {{Bank Marketing}},
  year         = {2012},
  howpublished = {UCI Machine Learning Repository},
  url          = {https://archive.ics.uci.edu/dataset/222/bank+marketing},
  doi          = {10.24432/C5K306}
}

@misc{misc_statlog_(german_credit_data)_144,
  author       = {Hofmann,Hans},
  title        = {{Statlog (German Credit Data)}},
  year         = {1994},
  howpublished = {UCI Machine Learning Repository},
  url          = {https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data},
  doi          = {10.24432/C5NC77}
}

@article{Kamishima2012,
abstract = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.},
author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
doi = {10.1007/978-3-642-33486-3},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamishima et al. - 2012 - Fairness-aware classifier with prejudice remover regularizer.pdf:pdf},
isbn = {9783642334856},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {classification,discrimination,fairness,information theory,logistic regression,social responsibility},
mendeley-groups = {Fairness},
number = {PART 2},
pages = {35--50},
title = {{Fairness-aware classifier with prejudice remover regularizer}},
volume = {7524 LNAI},
year = {2012}
}

@article{ElisaCelis2019,
abstract = {Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex “linear fractional” constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.},
archivePrefix = {arXiv},
arxivId = {1806.06055},
author = {{Elisa Celis}, L. and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
doi = {10.1145/3287560.3287586},
eprint = {1806.06055},
file = {:home/canalli/Downloads/1806.06055.pdf:pdf},
isbn = {9781450361255},
journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
keywords = {Algorithmic Fairness,Classification},
mendeley-groups = {Fairness},
pages = {319--328},
title = {{Classification with fairness constraints: A meta-algorithm with provable guarantees}},
year = {2019}
}



@misc{TwAI_Europe,
    title = {Ethics guidelines for trustworthy AI},
    author = {AI HLEG},
    month = apr,
    year = {2019},
    url = {https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai}
}

@misc{aif360-oct-2018,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    doi = {10.48550/arXiv.1810.01943},
    url = {https://www.ibm.com/opensource/open/projects/ai-fairness-360/}
}

@misc{gad2021pygad,

      title={PyGAD: An Intuitive Genetic Algorithm Python Library},

      author={Ahmed Fawzy Gad},

      year={2021},

      eprint={2106.06158},

      archivePrefix={arXiv},

      primaryClass={cs.NE}

}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Kasirzadeh2021,
abstract = {The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.},
archivePrefix = {arXiv},
arxivId = {2102.05085},
author = {Kasirzadeh, Atoosa and Smart, Andrew},
doi = {10.1145/3442188.3445886},
eprint = {2102.05085},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kasirzadeh, Smart - 2021 - The use and misuse of counterfactuals in ethical machine learning.pdf:pdf},
isbn = {9781450383097},
journal = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
keywords = {Algorithmic Fairness,Counterfactuals,Ethical AI,Ethics of AI,Explainable AI,Explanation,Fairness,Machine learning,Philosophy,Philosophy of AI,Social category,Social kind,Social ontology},
mendeley-groups = {Fairness},
pages = {228--236},
title = {{The use and misuse of counterfactuals in ethical machine learning}},
year = {2021}
}




@article{Hort2023,
abstract = {This paper provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.},
archivePrefix = {arXiv},
arxivId = {2207.07068},
author = {Hort, Max and Chen, Zhenpeng and Zhang, Jie M. and Harman, Mark and Sarro, Federica},
doi = {10.1145/3631326},
eprint = {2207.07068},
file = {:home/canalli/Downloads/2207.07068.pdf:pdf},
journal = {ACM Journal on Responsible Computing},
mendeley-groups = {Fairness},
pages = {1--52},
title = {{Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey}},
year = {2023}
}


@article{Frenay2014,
abstract = {Label noise is an important issue in classification, with many potential negative consequences. For example, the accuracy of predictions may decrease, whereas the complexity of inferred models and the number of necessary training samples may increase. Many works in the literature have been devoted to the study of label noise and the development of techniques to deal with label noise. However, the field lacks a comprehensive survey on the different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to fill this gap. First, the definitions and sources of label noise are considered and a taxonomy of the types of label noise is proposed. Second, the potential consequences of label noise are discussed. Third, label noise-robust, label noise cleansing, and label noise-tolerant algorithms are reviewed. For each category of approaches, a short discussion is proposed to help the practitioner to choose the most suitable technique in its own particular field of application. Eventually, the design of experiments is also discussed, what may interest the researchers who would like to test their own algorithms. In this paper, label noise consists of mislabeled instances: no additional information is assumed to be available like e.g., confidences on labels.},
author = {Fr{\'{e}}nay, Beno{\^{i}}t and Verleysen, Michel},
doi = {10.1109/TNNLS.2013.2292894},
file = {:C\:/Users/ygorc/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fr{\'{e}}nay, Verleysen - 2014 - Classification in the presence of label noise A survey(2).pdf:pdf},
isbn = {2162-237X VO - 25},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Class noise,classification,label noise,mislabeling,robust methods,survey.},
mendeley-groups = {Noise},
number = {5},
pages = {845--869},
pmid = {24808033},
title = {{Classification in the presence of label noise: A survey}},
volume = {25},
year = {2014}
}

@article{Hickey1996,
abstract = {The means of evaluating, using artificial data, algorithms, such as ID3, which learn concepts from examples is enhanced and referred to as the method of artificial universes. The central notions are that of a class model and its associated representations in which a class attribute is treated as a dependent variable with description attributes functioning as the independent variables. The nature of noise in the model is discussed and modelled using information-theoretic ideas especially that of majorisation. The notion of an irrelevant attribute is also considered. The ideas are illustrated through the construction of a small universe which is then altered to increase noise. Learning curves for ID3 used on data generated from these universes are estimated from trials. These show that increasing noise has a detrimental effect on learning.},
author = {Hickey, Ray J},
doi = {http://dx.doi.org/10.1016/0004-3702(94)00094-8},
file = {:home/canalli/Downloads/1-s2.0-0004370294000948-main.pdf:pdf},
issn = {0004-3702},
journal = {Artificial Intelligence},
mendeley-groups = {Noise},
month = {apr},
number = {1–2},
pages = {157--179},
title = {{Noise modelling and evaluating learning from examples}},
url = {http://www.sciencedirect.com/science/article/pii/0004370294000948},
volume = {82},
year = {1996}
}

@article{Quinlan1986,
abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
author = {Quinlan, J. R.},
doi = {10.1007/BF00116251},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan - 1986 - Induction of decision trees.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning 1986 1:1},
keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling},
mendeley-groups = {Noise},
month = {mar},
number = {1},
pages = {81--106},
publisher = {Springer},
title = {{Induction of decision trees}},
url = {https://link.springer.com/article/10.1007/BF00116251},
volume = {1},
year = {1986}
}

@inproceedings{Prost2021,
author = {Prost, Flavien and Awasthi, Pranjal and Blumm, Nick and Kumthekar, Aditee and Potter, Trevor and Wei, Li and Wang, Xuezhi and Chi, Ed H. and Chen, Jilin and Beutel, Alex},
title = {Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462603},
doi = {10.1145/3461702.3462603},
abstract = {In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {873–883},
numpages = {11},
keywords = {statistical parity, ml fairness, noisy covariates},
location = {Virtual Event, USA},
series = {AIES '21}
}

@misc{chatterjee2020new,
author = {Chatterjee, Sourav},
doi = {10.1080/01621459.2020.1758115},
journal = {Journal of the American Statistical Association},
number = {536},
pages = {2009--2022},
publisher = {Taylor \& Francis},
title = {{A New Coefficient of Correlation}},
url = {https://doi.org/10.1080/01621459.2020.1758115},
volume = {116},
year = {2021}
}


@article{Mehrotra2021,
abstract = {Subset selection algorithms are ubiquitous in AI-driven applications,
including, online recruiting portals and image search engines, so it is
imperative that these tools are not discriminatory on the basis of protected
attributes such as gender or race. Currently, fair subset selection algorithms
assume that the protected attributes are known as part of the dataset. However,
protected attributes may be noisy due to errors during data collection or if
they are imputed (as is often the case in real-world settings). While a wide
body of work addresses the effect of noise on the performance of machine
learning algorithms, its effect on fairness remains largely unexamined. We find
that in the presence of noisy protected attributes, in attempting to increase
fairness without considering noise, one can, in fact, decrease the fairness of
the result! Towards addressing this, we consider an existing noise model in which there
is probabilistic information about the protected attributes (e.g., [58, 34, 20,
46]), and ask is fair selection possible under noisy conditions? We formulate a
``denoised'' selection problem which functions for a large class of fairness
metrics; given the desired fairness goal, the solution to the denoised problem
violates the goal by at most a small multiplicative amount with high
probability. Although this denoised problem turns out to be NP-hard, we give a
linear-programming based approximation algorithm for it. We evaluate this
approach on both synthetic and real-world datasets. Our empirical results show
that this approach can produce subsets which significantly improve the fairness
metrics despite the presence of noisy protected attributes, and, compared to
prior noise-oblivious approaches, has better Pareto-tradeoffs between utility
and fairness.},
archivePrefix = {arXiv},
arxivId = {2011.04219},
author = {Mehrotra, Anay and Celis, L. Elisa},
doi = {10.48550/arxiv.2011.04219},
eprint = {2011.04219},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehrotra, Celis - 2020 - Mitigating Bias in Set Selection with Noisy Protected Attributes.pdf:pdf},
isbn = {9781450383097},
journal = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
mendeley-groups = {Fairness},
month = {nov},
pages = {237--248},
publisher = {Association for Computing Machinery, Inc},
title = {{Mitigating Bias in Set Selection with Noisy Protected Attributes}},
url = {https://arxiv.org/abs/2011.04219v2},
year = {2021}
}

@inproceedings{Patrini2016,
author = {Patrini, Giorgio and Nielsen, Frank and Nock, Richard and Carioni, Marcello},
title = {Loss Factorization, Weakly Supervised Learning and Label Noise Robustness},
year = {2016},
publisher = {JMLR.org},
abstract = {We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator -- the focal quantity of this work -- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {708–717},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{Patrini2017,
   abstract = {We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures - stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers - demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.},
   author = {Giorgio Patrini and Alessandro Rozza and Aditya Krishna Menon and Richard Nock and Lizhen Qu},
   doi = {10.1109/CVPR.2017.240},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   pages = {2233-2241},
   title = {Making deep neural networks robust to label noise: A loss correction approach},
   volume = {2017-Janua},
   year = {2017},
}


@inproceedings{Goh2016,
 author = {Goh, Gabriel and Cotter, Andrew and Gupta, Maya and Friedlander, Michael P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Satisfying Real-world Goals with Dataset Constraints},
 url = {https://proceedings.neurips.cc/paper\_files/paper/2016/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf},
 volume = {29},
 year = {2016}
}


@article{Calders2010,
   abstract = {In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modi-fying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model param-eters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data.},
   author = {Toon Calders and Sicco Verwer},
   doi = {10.1007/s10618-010-0190-x},
   issn = {13845810},
   issue = {2},
   journal = {Data Mining and Knowledge Discovery},
   keywords = {Discrimination-aware classification,Expectation maximization,Naive bayes},
   pages = {277-292},
   title = {Three naive Bayes approaches for discrimination-free classification},
   volume = {21},
   year = {2010},
}


@InProceedings{Woodworth2017,
  title = 	 {Learning Non-Discriminatory Predictors},
  author = 	 {Woodworth, Blake and Gunasekar, Suriya and Ohannessian, Mesrob I. and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {1920--1953},
  year = 	 {2017},
  editor = 	 {Kale, Satyen and Shamir, Ohad},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--10 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v65/woodworth17a/woodworth17a.pdf},
  url = 	 {https://proceedings.mlr.press/v65/woodworth17a.html},
  abstract = 	 {We consider learning a predictor which is non-discriminatory with respect to a “protected attribute” according to the notion of “equalized odds” proposed by Hardt et al. (2016).  We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally.  We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.}
}



@article{Liu2022,
   abstract = {In the application of machine learning to real-life decision-making systems, e.g., credit scoring and criminal justice, the prediction outcomes might discriminate against people with sensitive attributes, leading to unfairness. The commonly used strategy in fair machine learning is to include fairness as a constraint or a penalization term in the minimization of the prediction loss, which ultimately limits the information given to decision-makers. In this paper, we introduce a new approach to handle fairness by formulating a stochastic multi-objective optimization problem for which the corresponding Pareto fronts uniquely and comprehensively define the accuracy-fairness trade-offs. We have then applied a stochastic approximation-type method to efficiently obtain well-spread and accurate Pareto fronts, and by doing so we can handle training data arriving in a streaming way.},
   author = {Suyun Liu and Luis Nunes Vicente},
   doi = {10.1007/s10287-022-00425-z},
   issn = {16196988},
   issue = {3},
   journal = {Computational Management Science},
   month = {7},
   pages = {513-537},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Accuracy and fairness trade-offs in machine learning: a stochastic multi-objective approach},
   volume = {19},
   year = {2022},
}


@article{Wei2022,
   abstract = {Algorithmic fairness seeks to identify and correct sources of bias in machine learning algorithms. Confoundingly, ensuring fairness often comes at the cost of accuracy. We provide formal tools in this work for reconciling this fundamental tension in algorithm fairness. Specifically, we put to use the concept of Pareto optimality from multiobjective optimization and seek the fairness-accuracy Pareto front of a neural network classifier. We demonstrate that many existing algorithmic fairness methods are performing the so-called linear scalarization scheme, which has severe limitations in recovering Pareto optimal solutions. We instead apply the Chebyshev scalarization scheme which is provably superior theoretically and no more computationally burdensome at recovering Pareto optimal solutions compared to the linear scheme.},
   author = {Susan Wei and Marc Niethammer},
   doi = {10.1002/SAM.11560},
   issn = {19321872},
   issue = {3},
   journal = {Statistical Analysis and Data Mining},
   keywords = {Pareto front,Pareto optimality,fairness,multiobjective optimization,neural network},
   month = {6},
   pages = {287-302},
   publisher = {John Wiley and Sons Inc},
   title = {The fairness-accuracy Pareto front},
   volume = {15},
   year = {2022},
}


@article{Xu2019,
   abstract = {How to achieve fairness is important for next generation machine learning. Two tasks that are equally important in fair machine learning are how to obtain fair datasets and how to build fair classifiers. In this work, we propose a new generative adversarial network (GAN) model for fair machine learning, named FairGAN+. FairGAN+ contains a generator to generate close-to-real samples, a classifier to predict class labels and three discriminators to assist adversarial learning. FairGAN+ simultaneously achieves fair data generation and classification by co-training the generative model and the classifier through joint adversarial games with the discriminators. Evaluations on real world data show the effectiveness of FairGAN+ on both fair data generation and fair classification.},
   author = {Depeng Xu and Shuhan Yuan and Lu Zhang and Xintao Wu},
   doi = {10.1109/BigData47090.2019.9006322},
   isbn = {9781728108582},
   journal = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
   keywords = {fair classification,fair data generation,fairness-aware learning,generative adversarial networks},
   pages = {1401-1406},
   publisher = {IEEE},
   title = {FairGAN+: Achieving Fair Data Generation and Classification through Generative Adversarial Nets},
   year = {2019},
}

@article{Kearns2017,
   abstract = {The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent "fairness gerrymandering", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. We then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.},
   author = {Michael Kearns and Seth Neel and Aaron Roth and Zhiwei Steven Wu},
   month = {11},
   title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
   url = {http://arxiv.org/abs/1711.05144},
   year = {2017},
}

@inproceedings{Kearns2018,
author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
title = {An Empirical Study of Rich Subgroup Fairness for Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287592},
doi = {10.1145/3287560.3287592},
abstract = {Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {100–109},
numpages = {10},
keywords = {Algorithmic Bias, Subgroup Fairness, Fairness Auditing, Fair Classification},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{chicco2020advantages,
  title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BMC genomics},
  volume={21},
  number={1},
  pages={1--13},
  year={2020},
  publisher={BioMed Central}
}

@inproceedings{dror2019deep,
  author    = {Rotem Dror and
               Segev Shlomov and
               Roi Reichart},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {Deep Dominance - How to Properly Compare Deep Neural Models},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {2773--2785},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1266},
  doi       = {10.18653/v1/p19-1266},
  timestamp = {Tue, 28 Jan 2020 10:27:52 +0100},
}

@incollection{del2018optimal,
  title={An optimal transportation approach for assessing almost stochastic order},
  author={Del Barrio, Eustasio and Cuesta-Albertos, Juan A and Matr{\'a}n, Carlos},
  booktitle={The Mathematics of the Uncertain},
  pages={33--44},
  year={2018},
  publisher={Springer}
}

@article{Hutchinson2019,
  title={50 Years of Test (Un)fairness: Lessons for Machine Learning},
  author={Ben Hutchinson and Margaret Mitchell},
  journal={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:53782832}
}



@InProceedings{kearns18a,
  title = 	 {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author =       {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2564--2572},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kearns18a.html},
  abstract = 	 {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.}
}

@INPROCEEDINGS{Kamiran2012b,
  author={Kamiran, Faisal and Karim, Asim and Zhang, Xiangliang},
  booktitle={2012 IEEE 12th International Conference on Data Mining}, 
  title={Decision Theory for Discrimination-Aware Classification}, 
  year={2012},
  volume={},
  number={},
  pages={924-929},
  keywords={Accuracy;Probabilistic logic;Standards;Communities;Data mining;Decision trees;Logistics;social discrimination;classification;decision theory;ensembles},
  doi={10.1109/ICDM.2012.45}}



@InProceedings{Zemel2013,
  title = 	 {Learning Fair Representations},
  author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}


@inproceedings{Dwork2011,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@InProceedings{Zafar2017b,
  title = 	 {{Fairness Constraints: Mechanisms for Fair Classification}},
  author = 	 {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {962--970},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/zafar17a.html},
  abstract = 	 {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.}
}

@inproceedings{Zafar2017a,
author = {Zafar, Muhammad Bilal and Valera, Isabel and Gomez Rodriguez, Manuel and Gummadi, Krishna P.},
title = {Fairness Beyond Disparate Treatment \& Disparate Impact: Learning Classification without Disparate Mistreatment},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052660},
doi = {10.1145/3038912.3052660},
abstract = {Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1171–1180},
numpages = {10},
keywords = {fair classification, discrimination in decision making, machine learning and law, fair decision making, algorithmic decision making},
location = {Perth, Australia},
series = {WWW '17}
}

@article{Zhang2022,
abstract = {A growing specter in the rise of machine learning is whether the decisions made by machine learning models are fair. While research is already underway to formalize a machine-learning concept of fairness and to design frameworks for building fair models with sacrifice in accuracy, most are geared toward either supervised or unsupervised learning. Yet two observations inspired us to wonder whether semi-supervised learning might be useful to solve discrimination problems. First, previous study showed that increasing the size of the training set may lead to a better trade-off between fairness and accuracy. Second, the most powerful models today require an enormous of data to train which, in practical terms, is likely possible from a combination of labeled and unlabeled data. Hence, in this paper, we present a framework of fair semi-supervised learning in the pre-processing phase, including pseudo labeling to predict labels for unlabeled data, a re-sampling method to obtain multiple fair datasets and lastly, ensemble learning to improve accuracy and decrease discrimination. A theoretical decomposition analysis of bias, variance and noise highlights the different sources of discrimination and the impact they have on fairness in semi-supervised learning. A set of experiments on real-world and synthetic datasets show that our method is able to use unlabeled data to achieve a better trade-off between accuracy and discrimination.},
archivePrefix = {arXiv},
arxivId = {2009.12040},
author = {Zhang, Tao and Zhu, Tianqing and Li, Jing and Han, Mengde and Zhou, Wanlei and Yu, Philip S.},
doi = {10.1109/TKDE.2020.3002567},
eprint = {2009.12040},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2022 - Fairness in Semi-Supervised Learning Unlabeled Data Help to Reduce Discrimination.pdf:pdf},
issn = {15582191},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Fairness,discrimination,machine learning,semi-supervised learning},
mendeley-groups = {Fairness},
month = {apr},
number = {4},
pages = {1763--1774},
publisher = {IEEE Computer Society},
title = {{Fairness in Semi-Supervised Learning: Unlabeled Data Help to Reduce Discrimination}},
volume = {34},
year = {2022}
}

@article{Weinberg2022,
abstract = {This survey article assesses and compares existing critiques of current fairness-enhancing technical interventions in machine learning (ML) that draw from a range of non-computing disciplines, including philosophy, feminist studies, critical race and ethnic studies, legal studies, anthropology, and science and technology studies. It bridges epistemic divides in order to offer an interdisciplinary understanding of the possibilities and limits of hegemonic computational approaches to ML fairness for producing just outcomes for society's most marginalized. The article is organized according to nine major themes of critique wherein these different fields intersect: 1) how "fairness" in AI fairness research gets defined; 2) how problems for AI systems to address get formulated; 3) the impacts of abstraction on how AI tools function and its propensity to lead to technological solutionism; 4) how racial classification operates within AI fairness research; 5) the use of AI fairness measures to avoid regulation and engage in ethics washing; 6) an absence of participatory design and democratic deliberation in AI fairness considerations; 7) data collection practices that entrench “bias,” are non-consensual, and lack transparency; 8) the predatory inclusion of marginalized groups into AI systems; and 9) a lack of engagement with AI's long-term social and ethical outcomes. Drawing from these critiques, the article concludes by imagining future ML fairness research directions that actively disrupt entrenched power dynamics and structural injustices in society.},
author = {Weinberg, Lindsay},
doi = {10.1613/jair.1.13196},
file = {:home/canalli/Downloads/13196-Article (PDF)-30499-1-10-20220506.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
mendeley-groups = {Fairness},
pages = {75--109},
title = {{Rethinking Fairness: An Interdisciplinary Survey of Critiques of Hegemonic ML Fairness Approaches}},
volume = {74},
year = {2022}
}

@article{AlerTubella2022,
abstract = {The importance of fairness in machine learning models is widely acknowledged, and ongoing academic debate revolves around how to determine the appropriate fairness definition, and how to tackle the trade-off between fairness and model performance. In this paper we argue that besides these concerns, there can be ethical implications behind seemingly purely technical choices in fairness interventions in a typical model development pipeline. As an example we show that the technical choice between in-processing and post-processing is not necessarily value-free and may have serious implications in terms of who will be affected by the specific fairness intervention. The paper reveals how assessing the technical choices in terms of their ethical consequences can contribute to the design of fair models and to the related societal discussions.},
author = {{Aler Tubella}, Andrea and Barsotti, Flavia and Ko{\c{c}}er, R{\"{u}}ya G{\"{o}}khan and Mendez, Julian Alfredo},
doi = {10.1007/S10676-022-09636-Z/TABLES/4},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aler Tubella et al. - 2022 - Ethical implications of fairness interventions what might be hidden behind engineering choices.pdf:pdf},
issn = {15728439},
journal = {Ethics and Information Technology},
keywords = {AI Ethics,Bias mitigation,Fairness,Responsible AI},
month = {mar},
number = {1},
pages = {1--11},
publisher = {Springer Science and Business Media B.V.},
title = {{Ethical implications of fairness interventions: what might be hidden behind engineering choices?}},
url = {https://link.springer.com/article/10.1007/s10676-022-09636-z},
volume = {24},
year = {2022}
}
@article{Liu_Wang_Wang_Wang_Su_Gao_2023,
author = {Liu, Tianci and Wang, Haoyu and Wang, Yaqing and Wang, Xiaoqian and Su, Lu and Gao, Jing},
doi = {10.1609/aaai.v37i12.26677},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {12},
pages = {14338--14346},
title = {{SimFair: A Unified Framework for Fairness-Aware Multi-Label Classification}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/26677},
volume = {37},
year = {2023}
}

@article{KIM2023231,
abstract = {Existing classification models often output discriminatory results since they learn the target attribute without addressing data imbalance with respect to the protected attributes (e.g., gender). The models tend to focus on learning toward demographic groups containing the larger number of training samples, which consequently leads to training loss discrepancy between the groups. Our work focuses on addressing the occurrence of training loss discrepancy between the groups to improve the model's fairness. To this end, we firstly define the target-protected group using the target and protected attribute labels and observe the group-wise training loss in terms of previous fairness approaches. From the observation, we figure out that balancing the total loss across all the groups allows to mitigate fairness issue significantly, and meanwhile, only considering the sample size of each group to obtain a balanced mini-batch is not enough for mitigating fairness. Motivated by the observations, we propose a fairness-aware batch sampling scheme that adaptively updates batch sampling probability (BSP) and constructs a fairness-aware mini-batch from the model's point of view. Our key idea is to balance the training losses via training with fairness-aware mini-batch. Through extensive experiments on two facial attribute benchmark datasets and one tabular dataset, our simple and effective sampling strategy achieves superior improvement in terms of two standard fairness metrics. We validate our algorithm with various experimental settings (e.g, multi-attribute classification, binary classification with multiple protected attributes). Moreover, we introduce a new metric for measuring the trade-off between fairness and classification performance. On this metric, our algorithm also achieves the best trade-off performance.},
author = {Kim, Dohyung and Park, Sungho and Hwang, Sunhee and Byun, Hyeran},
doi = {https://doi.org/10.1016/j.neucom.2022.11.018},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = { Classification, Data imbalance, Deep neural network, Resampling,Fairness AI},
pages = {231--241},
title = {{Fair classification by loss balancing via fairness-aware batch sampling}},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222013984},
volume = {518},
year = {2023}
}

@inproceedings{KhaliliZA23,
  author       = {Mohammad Mahdi Khalili and
                  Xueru Zhang and
                  Mahed Abroshan},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Loss Balancing for Fair Supervised Learning},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {16271--16290},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/khalili23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/KhaliliZA23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DAloisio2023,
abstract = {Nowadays assuring that search and recommendation systems are fair and do not apply discrimination among any kind of population has become of paramount importance. This is also highlighted by some of the sustainable development goals proposed by the United Nations. Those systems typically rely on machine learning algorithms that solve the classification task. Although the problem of fairness has been widely addressed in binary classification, unfortunately, the fairness of multi-class classification problem needs to be further investigated lacking well-established solutions. For the aforementioned reasons, in this paper, we present the Debiaser for Multiple Variables (DEMV), an approach able to mitigate unbalanced groups bias (i.e., bias caused by an unequal distribution of instances in the population) in both binary and multi-class classification problems with multiple sensitive variables. The proposed method is compared, under several conditions, with a set of well-established baselines using different categories of classifiers. At first we conduct a specific study to understand which is the best generation strategies and their impact on DEMV's ability to improve fairness. Then, we evaluate our method on a heterogeneous set of datasets and we show how it overcomes the established algorithms of the literature in the multi-class classification setting and in the binary classification setting when more than two sensitive variables are involved. Finally, based on the conducted experiments, we discuss strengths and weaknesses of our method and of the other baselines.},
author = {D'Aloisio, Giordano and D'Angelo, Andrea and {Di Marco}, Antinisca and Stilo, Giovanni},
doi = {10.1016/j.ipm.2022.103226},
file = {:home/canalli/Downloads/1-s2.0-S0306457322003272-main.pdf:pdf},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Bias and Fairness,Equality,Machine learning,Multi-class classification,Preprocessing algorithm},
mendeley-groups = {Fairness/multiclass},
number = {2},
pages = {103226},
publisher = {Elsevier Ltd},
title = {{Debiaser for Multiple Variables to enhance fairness in classification tasks}},
url = {https://doi.org/10.1016/j.ipm.2022.103226},
volume = {60},
year = {2023}
}

@article{Liang2023,
abstract = {Modern machine learning (ML) models are becoming increasingly popular and are widely used in decision-making systems. However, studies have shown critical issues of ML discrimination and unfairness, which hinder their adoption on high-stake applications. Recent research on fair classifiers has drawn significant attention to developing effective algorithms to achieve fairness and good classification performance. Despite the great success of these fairness-aware machine learning models, most of the existing models require sensitive attributes to pre-process the data, regularize the model learning or post-process the prediction to have fair predictions. However, sensitive attributes are often incomplete or even unavailable due to privacy, legal or regulation restrictions. Though we lack the sensitive attribute for training a fair model in the target domain, there might exist a similar domain that has sensitive attributes. Thus, it is important to exploit auxiliary information from a similar domain to help improve fair classification in the target domain. Therefore, in this paper, we study a novel problem of exploring domain adaptation for fair classification. We propose a new framework that can learn to adapt the sensitive attributes from a source domain for fair classification in the target domain. Extensive experiments on real-world datasets illustrate the effectiveness of the proposed model for fair classification, even when no sensitive attributes are available in the target domain.},
author = {Liang, Yueqing and Chen, Canyu and Tian, Tian and Shu, Kai},
doi = {10.3389/fdata.2022.1049565},
issn = {2624-909X},
journal = {Frontiers in Big Data},
title = {{Fair classification via domain adaptation: A dual adversarial learning approach}},
url = {https://www.frontiersin.org/articles/10.3389/fdata.2022.1049565},
volume = {5},
year = {2023}
}

@article{ZhangZLZY23,
  author       = {Tao Zhang and
                  Tianqing Zhu and
                  Jing Li and
                  Wanlei Zhou and
                  Philip S. Yu},
  title        = {Revisiting model fairness via adversarial examples},
  journal      = {Knowl. Based Syst.},
  volume       = {277},
  pages        = {110777},
  year         = {2023},
  url          = {https://doi.org/10.1016/j.knosys.2023.110777},
  doi          = {10.1016/J.KNOSYS.2023.110777},
  timestamp    = {Sun, 24 Sep 2023 15:45:50 +0200},
  biburl       = {https://dblp.org/rec/journals/kbs/ZhangZLZY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Zeming2023,
  author       = {Zeming Wei and
                  Yifei Wang and
                  Yiwen Guo and
                  Yisen Wang},
  title        = {{CFA:} Class-wise Calibrated Fair Adversarial Training},
  journal      = {CoRR},
  volume       = {abs/2303.14460},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.14460},
  doi          = {10.48550/ARXIV.2303.14460},
  eprinttype    = {arXiv},
  eprint       = {2303.14460},
  timestamp    = {Tue, 16 May 2023 16:54:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-14460.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Yuchen2023,
  author       = {Yuchen Ma and
                  Dennis Frauen and
                  Valentyn Melnychuk and
                  Stefan Feuerriegel},
  title        = {Counterfactual Fairness for Predictions using Generative Adversarial
                  Networks},
  journal      = {CoRR},
  volume       = {abs/2310.17687},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.17687},
  doi          = {10.48550/ARXIV.2310.17687},
  eprinttype    = {arXiv},
  eprint       = {2310.17687},
  timestamp    = {Thu, 02 Nov 2023 17:30:29 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-17687.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MousaviMD23,
  author       = {Seyed Ali Mousavi and
                  Hamid Mousavi and
                  Masoud Daneshtalab},
  editor       = {Alberto Abell{\'{o}} and
                  Panos Vassiliadis and
                  Oscar Romero and
                  Robert Wrembel},
  title        = {{FARMUR:} Fair Adversarial Retraining to Mitigate Unfairness in Robustness},
  booktitle    = {Advances in Databases and Information Systems - 27th European Conference,
                  {ADBIS} 2023, Barcelona, Spain, September 4-7, 2023, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {13985},
  pages        = {133--145},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-42914-9\_10},
  doi          = {10.1007/978-3-031-42914-9\_10},
  timestamp    = {Mon, 04 Sep 2023 20:40:44 +0200},
  biburl       = {https://dblp.org/rec/conf/adbis/MousaviMD23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{GrariLD23,
  author       = {Vincent Grari and
                  Sylvain Lamprier and
                  Marcin Detyniecki},
  title        = {Adversarial learning for counterfactual fairness},
  journal      = {Mach. Learn.},
  volume       = {112},
  number       = {3},
  pages        = {741--763},
  year         = {2023},
  url          = {https://doi.org/10.1007/s10994-022-06206-8},
  doi          = {10.1007/S10994-022-06206-8},
  timestamp    = {Sat, 11 Mar 2023 00:13:29 +0100},
  biburl       = {https://dblp.org/rec/journals/ml/GrariLD23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ghosh2023,
author = {Ghosh, Avijit and Kvitca, Pablo and Wilson, Christo},
title = {When Fair Classification Meets Noisy Protected Attributes},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604707},
doi = {10.1145/3600211.3604707},
abstract = {The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all. To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-unaware algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-unaware and noise-tolerant fair classifiers can potentially achieve similar level of performance as attribute-reliant algorithms, even when protected attributes are noisy. However, implementing them in practice requires careful nuance. Our study provides insights into the practical implications of using fair classification algorithms in scenarios where protected attributes are noisy or partially available.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {679–690},
numpages = {12},
keywords = {protected attributes, fairness, evaluation, classification},
location = {<conf-loc>, <city>Montr\'{e}al</city>, <state>QC</state>, <country>Canada</country>, </conf-loc>},
series = {AIES '23}
}

@article{Alves2023,
title = {Survey on fairness notions and related tensions},
journal = {EURO Journal on Decision Processes},
volume = {11},
pages = {100033},
year = {2023},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2023.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2193943823000067},
author = {Guilherme Alves and Fabien Bernier and Miguel Couceiro and Karima Makhlouf and Catuscia Palamidessi and Sami Zhioua},
keywords = {Fairness notion, Tension within fairness, Unfairness mitigation},
abstract = {Automated decision systems are increasingly used to take consequential decisions in problems such as job hiring and loan granting with the hope of replacing subjective human decisions with objective machine learning (ML) algorithms. However, ML-based decision systems are prone to bias, which results in yet unfair decisions. Several notions of fairness have been defined in the literature to capture the different subtleties of this ethical and social concept (e.g., statistical parity, equal opportunity, etc.). Fairness requirements to be satisfied while learning models created several types of tensions among the different notions of fairness and other desirable properties such as privacy and classification accuracy. This paper surveys the commonly used fairness notions and discusses the tensions among them with privacy and accuracy. Different methods to address the fairness-accuracy trade-off (classified into four approaches, namely, pre-processing, in-processing, post-processing, and hybrid) are reviewed. The survey is consolidated with experimental analysis carried out on fairness benchmark datasets to illustrate the relationship between fairness measures and accuracy in real-world scenarios.}
}

@article{caton2023,
author = {Caton, Simon and Haas, Christian},
title = {Fairness in Machine Learning: A Survey},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616865},
doi = {10.1145/3616865},
abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {aug},
keywords = {machine learning, transparency, accountability, fairness}
}

% pubman genre = conference-paper
@inproceedings{Grgi2016,
title = {{The Case for Process Fairness in Learning: {\textbraceleft}F{\textbraceright}eature Selection for Fair Decision Making}},
author = {Grgi{\'c}-Hla{\v{c}}a, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P. and Weller, Adrian},
language = {eng},
year = {2016},
booktitle = {{Symposium on Machine Learning and the Law at the 29th Conference on Neural Information Processing Systems}},
address = {Barcelona, Spain},
note = {Symposium on Machine Learning and the Law},
}


@article{Mehrabi2019,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {machine learning, Fairness and bias in artificial intelligence, representation learning, natural language processing, deep learning}
}

@article{Jacobs2021,
abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them-i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different oper-ationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
author = {Jacobs, Abigail Z and Wallach, Hanna},
doi = {10.1145/3442188.3445901},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobs, Wallach - 2021 - Measurement and Fairness.pdf:pdf},
isbn = {9781450383097},
keywords = {CCS CONCEPTS • General and reference → Measurement,Relia-bility KEYWORDS measurement, construct validity, construct reliability, fairness,Validation},
mendeley-groups = {Fairness},
number = {21},
publisher = {Canada},
title = {{Measurement and Fairness}},
url = {https://doi.org/10.1145/3442188.3445901},
volume = {11},
year = {2021}
}

@article{Wang2020,
archivePrefix = {arXiv},
arxivId = {2002.09343},
author = {Wang, Serena and Guo, Wenshuo and Narasimhan, Harikrishna and Cotter, Andrew and Gupta, Maya and Jordan, Michael I.},
doi = {10.48550/arxiv.2002.09343},
eprint = {2002.09343},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Robust Optimization for Fairness with Noisy Protected Groups.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Noise,Fairness},
month = {feb},
publisher = {Neural information processing systems foundation},
title = {{Robust Optimization for Fairness with Noisy Protected Groups}},
url = {https://arxiv.org/abs/2002.09343v3},
volume = {2020-December},
year = {2020}
}

@InProceedings{Fogliato2020,
  title = 	 {Fairness Evaluation in Presence of Biased Noisy Labels},
  author =       {Fogliato, Riccardo and Chouldechova, Alexandra and G'Sell, Max},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2325--2336},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/fogliato20a/fogliato20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/fogliato20a.html},
  abstract = 	 {Risk assessment tools are widely used around the country to inform decision making within the criminal justice system. Recently, considerable attention has been devoted to the question of whether such tools may suffer from racial bias. In this type of assessment, a fundamental issue is that the training and evaluation of the model is based on a variable (arrest) that may represent a noisy version of an unobserved outcome of more central interest (offense). We propose a sensitivity analysis framework for assessing how assumptions on the noise across groups affect the predictive bias properties of the risk assessment model as a predictor of reoffense. Our experimental results on two real world criminal justice data sets demonstrate how even small biases in the observed labels may call into question the conclusions of an analysis based on the noisy outcome. }
}

@article{Chouldechova2017,
author = {Chouldechova, Alexandra},
title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
journal = {Big Data},
volume = {5},
number = {2},
pages = {153-163},
year = {2017},
doi = {10.1089/big.2016.0047},
    note ={PMID: 28632438},

URL = { 
    
        https://doi.org/10.1089/big.2016.0047
    
    

},
eprint = { 
    
        https://doi.org/10.1089/big.2016.0047
    
    

}
,
    abstract = { Abstract Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance. }
}



@article{Kleinberg2017,
abstract = {Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
archivePrefix = {arXiv},
arxivId = {1609.05807},
author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
doi = {10.4230/LIPIcs.ITCS.2017.43},
eprint = {1609.05807},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kleinberg, Mullainathan, Raghavan - 2017 - Inherent trade-offs in the fair determination of risk scores.pdf:pdf},
isbn = {9783959770293},
issn = {18688969},
journal = {Leibniz International Proceedings in Informatics, LIPIcs},
keywords = {algorithmic fairness,calibration,risk tools},
mendeley-groups = {PESC-Disciplines/DM,Fairness},
pages = {1--23},
title = {{Inherent trade-offs in the fair determination of risk scores}},
volume = {67},
year = {2017}
}

@article{Saravanakumar2020,
abstract = {With the increasing pervasive use of machine learning in social and economic settings, there has been an interest in the notion of machine bias in the AI community. Models trained on historic data reflect biases that exist in society and propagated them to the future through their decisions. There are three prominent metrics of machine fairness used in the community, and it has been shown statistically that it is impossible to satisfy them all at the same time. This has led to an ambiguity with regards to the definition of fairness. In this report, a causal perspective to the impossibility theorem of fairness is presented along with a causal goal for machine fairness.},
archivePrefix = {arXiv},
arxivId = {2007.06024},
author = {Saravanakumar, Kailash Karthik},
eprint = {2007.06024},
file = {:home/canalli/Downloads/2007.06024.pdf:pdf},
mendeley-groups = {Fairness/impossibility theorem},
title = {{The Impossibility Theorem of Machine Fairness -- A Causal Perspective}},
url = {http://arxiv.org/abs/2007.06024},
year = {2020}
}

@article{Bell2023,
abstract = {The "impossibility theorem"- which is considered foundational in algorithmic fairness literature - asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner's perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when - and to what degree - fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.},
archivePrefix = {arXiv},
arxivId = {2302.06347},
author = {Bell, Andrew and Bynum, Lucius and Drushchak, Nazarii and Zakharchenko, Tetiana and Rosenblatt, Lucas and Stoyanovich, Julia},
doi = {10.1145/3593013.3594007},
eprint = {2302.06347},
file = {:home/canalli/Downloads/2302.06347.pdf:pdf},
isbn = {9781450372527},
journal = {ACM International Conference Proceeding Series},
keywords = {fairness,machine learning,public policy,responsible AI},
mendeley-groups = {Fairness/impossibility theorem},
number = {1},
pages = {400--422},
publisher = {Association for Computing Machinery},
title = {{The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice}},
volume = {1},
year = {2023}
}

@article{Beigang2023,
abstract = {In recent years, there has been a surge in research addressing the question which properties predictive algorithms ought to satisfy in order to be considered fair. Three of the most widely discussed criteria of fairness are the criteria called equalized odds, predictive parity, and counterfactual fairness. In this paper, I will present a new impossibility result involving these three criteria of algorithmic fairness. In particular, I will argue that there are realistic circumstances under which any predictive algorithm that satisfies counterfactual fairness will violate both other fairness criteria, that is, equalized odds and predictive parity. As will be shown, this impossibility result forces us to give up one of four intuitively plausible assumptions about algorithmic fairness. I will explain and motivate each of the four assumptions and discuss which of them can plausibly be given up in order to circumvent the impossibility.},
author = {Beigang, Fabian},
doi = {10.1007/s11023-023-09645-x},
file = {:home/canalli/Downloads/s11023-023-09645-x.pdf:pdf},
isbn = {1102302309},
issn = {15728641},
journal = {Minds and Machines},
keywords = {AI ethics,Algorithmic fairness,Causal modeling,Impossibility theorem,Machine learning},
mendeley-groups = {Fairness/impossibility theorem},
number = {4},
pages = {715--735},
publisher = {Springer Netherlands},
title = {{Yet Another Impossibility Theorem in Algorithmic Fairness}},
url = {https://doi.org/10.1007/s11023-023-09645-x},
volume = {33},
year = {2023}
}


@inproceedings{Ghazimatin2022,
author = {Ghazimatin, Azin and Kleindessner, Matthaus and Russell, Chris and Abedjan, Ziawasch and Golebiowski, Jacek},
title = {Measuring Fairness of Rankings under Noisy Sensitive Information},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534641},
doi = {10.1145/3531146.3534641},
abstract = {Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2263–2279},
numpages = {17},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@InProceedings{Celis2021,
  title = 	 {Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees},
  author =       {Celis, L. Elisa and Huang, Lingxiao and Keswani, Vijay and Vishnoi, Nisheeth K.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1349--1361},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/celis21a/celis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/celis21a.html},
  abstract = 	 {We present an optimization framework for learning a fair classifier in the presence of noisy perturbations in the protected attributes. Compared to prior work, our framework can be employed with a very general class of linear and linear-fractional fairness constraints, can handle multiple, non-binary protected attributes, and outputs a classifier that comes with provable guarantees on both accuracy and fairness. Empirically, we show that our framework can be used to attain either statistical rate or false positive rate fairness guarantees with a minimal loss in accuracy, even when the noise is large, in two real-world datasets.}
}


@article{Lamy2019,
   abstract = {Fairness-aware learning involves designing algorithms that do not
discriminate with respect to some sensitive feature (e.g., race or gender).
Existing work on the problem operates under the assumption that the sensitive
feature available in one's training sample is perfectly reliable. This
assumption may be violated in many real-world cases: for example, respondents
to a survey may choose to conceal or obfuscate their group identity out of fear
of potential discrimination. This poses the question of whether one can still
learn fair classifiers given noisy sensitive features. In this paper, we answer
the question in the affirmative: we show that if one measures fairness using
the mean-difference score, and sensitive features are subject to noise from the
mutually contaminated learning model, then owing to a simple identity we only
need to change the desired fairness-tolerance. The requisite tolerance can be
estimated by leveraging existing noise-rate estimators from the label noise
literature. We finally show that our procedure is empirically effective on two
case-studies involving sensitive feature censoring.},
   author = {Alexandre Lamy and Ziyuan Zhong and Nakul Verma and Aditya Krishna Menon},
   doi = {10.48550/arxiv.1901.10837},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {1},
   publisher = {Neural information processing systems foundation},
   title = {Noise-tolerant fair classification},
   volume = {32},
   url = {https://arxiv.org/abs/1901.10837v4},
   year = {2019},
}

@article{Wu2022,
   abstract = {With the widespread use of machine learning systems in our daily lives, it is important to consider fairness as a basic requirement when designing these systems, especially when the systems make life-changing decisions, e.g., COMPAS algorithm helps judges decide whether to release an offender. For another thing, due to the cheap but imperfect data collection methods, such as crowd-sourcing and web crawling, label noise is ubiquitous, which unfortunately makes fairness-aware algorithms even more prejudiced than fairness-unaware ones, and thereby harmful. To tackle these problems, we provide general frameworks for learning fair classifiers with instance-dependent label noise. For statistical fairness notions, we rewrite the classification risk and the fairness metric in terms of noisy data and thereby build robust classifiers. For the causality-based fairness notion, we exploit the internal causal structure of data to model the label noise and counterfactual fairness simultaneously. Experimental results demonstrate the effectiveness of the proposed methods on real-world datasets with controllable synthetic label noise.},
   author = {Songhua Wu and Bo Han and Yang Liu and Tongliang Liu and Bernhard Schölkopf and Caroline Uhler and Kun Zhang and S Wu and M Gong and B Han and Y Liu and T Liu},
   journal = {Proceedings of Machine Learning Research},
   keywords = {causal graph,counterfactual fairness,instance-dependent label noise},
   pages = {1-17},
   title = {Fair Classification with Instance-dependent Label Noise},
   volume = {140},
   url = {https://www.mturk.com/},
   year = {2022},
}

@article{Wang2021,
   abstract = {This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.},
   author = {Jialu Wang and Uc Santa Cruz Santa Cruz and Usa Yang Liu and Caleb Levy and Yang Liu},
   doi = {10.1145/3442188.3445915},
   isbn = {9781450383097},
   journal = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
   keywords = {Algorithmic fairness,Learning with noisy and biased labels,Machine learning},
   month = {3},
   pages = {526-536},
   publisher = {Association for Computing Machinery, Inc},
   title = {Fair classification with group-dependent label noise},
   url = {https://doi.org/10.1145/3442188.3445915},
   year = {2021},
}

@article{Verma2018,
   abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algo-rithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
   author = {Sahil Verma and Julia Rubin},
   doi = {10.1145/3194770.3194776},
   isbn = {9781450357463},
   journal = {IEEE/ACM International Workshop on Software Fairness},
   publisher = {ACM},
   title = {Fairness Definitions Explained},
   volume = {18},
   url = {https://doi.org/10.1145/3194770.3194776},
   year = {2018},
}

@inproceedings{Kusner2018,
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
title = {Counterfactual Fairness},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4069–4079},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{Burkart2021,
author = {Burkart, Nadia and Huber, Marco F.},
title = {A Survey on the Explainability of Supervised Machine Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12228},
doi = {10.1613/jair.1.12228},
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {245–317},
numpages = {73}
}

@inproceedings{Hutchinson2021,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {requirements engineering, machine learning, datasets},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@misc{Memarian2023,
   abstract = {Background: The use of Artificial Intelligence or AI is rising in higher education. With this rise, the morality of AI programs is being questioned. There is, as such, a need to understand how notions of Fairness, Accountability, Transparency, and Ethics or FATE are identified in the AI and higher education studies to date. Purpose: This systematic review paper aims to understand definitions and studies on FATE and AI in the higher education literature. The contribution of this work is to provide a summary of FATE development and the synthesis of the challenges and potentials of each of the reviewed studies. Method: A total of 33 publications from SCOPUS and Web of Science (WoS) were included in this systematic literature review. We examined definitions of FATE noted in the reviewed articles (may have been multiple in each study) and grouped them into descriptive (understandable by laypeople) and technical (containing jargon) definitions. We also examined the main FATE term studied in detail in each reviewed article and grouped them into qualitative and quantitative studies. Results: Findings show more descriptive definitions exist (especially for fairness) and similarly quantitative definitions mostly emerge for Fairness. Findings also show more quantitative studies exist (especially for fairness) and qualitative definitions mostly emerge for ethics. Generally, though, there are more definitions than relevant studies conducted in the literature. Conclusion: This systematic literature review offers a summary of definitions and studies conducted for FATE terms and AI in the higher education literature. Future work may benefit from bridging the gap between laypeople and experts by linking descriptive definitions with technical ones as well as qualitative studies with quantitative ones. Moreover, future work can study accountability and transparency further and make the study of FATE terms more longitudinal, open-access, and reproducible.},
   author = {Bahar Memarian and Tenzin Doleck},
   doi = {10.1016/j.caeai.2023.100152},
   issn = {2666920X},
   journal = {Computers and Education: Artificial Intelligence},
   keywords = {Accountability,Artificial intelligence,Ethics,Fairness,Higher education,Transparency},
   month = {1},
   publisher = {Elsevier B.V.},
   title = {Fairness, Accountability, Transparency, and Ethics (FATE) in Artificial Intelligence (AI) and higher education: A systematic review},
   volume = {5},
   year = {2023},
}


@inproceedings{Hardt2016,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of Opportunity in Supervised Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3323–3331},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{ochigame2018beyond,
  title={Beyond legitimation: rethinking fairness, interpretability, and accuracy in machine learning},
  author={Ochigame, Rodrigo and Barabas, Chelsea and Dinakar, Karthik and Virza, Madars and Ito, Joichi},
  booktitle={Machine Learning: The Debates workshop at the 35th International Conference on Machine Learning},
  year={2018}
}

@techreport{Corbett-Davies2018,
abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes-like race, gender, and their proxies-are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
archivePrefix = {arXiv},
arxivId = {1808.00023v2},
author = {Corbett-Davies, Sam and Goel, Sharad and Chohlas-Wood, Alex and Chouldechova, Alexandra and Feller, Avi and Huq, Aziz and Hardt, Moritz and Ho, Daniel E and Mitchell, Shira and Overgoor, Jan and Pierson, Emma and Shroff, Ravi},
eprint = {1808.00023v2},
file = {:home/canalli/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Corbett-Davies et al. - 2018 - The Measure and Mismeasure of Fairness A Critical Review of Fair Machine Learning.pdf:pdf},
keywords = {Algorithms,anti-classification,bias,calibration,classification parity,decision analysis,measurement error * We thank},
mendeley-groups = {Fairness},
title = {{The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning *}},
url = {https://arxiv.org/pdf/1808.00023.pdf},
year = {2018}
}

@INPROCEEDINGS{Cruz2021,
  author={F.Cruz, André and Saleiro, Pedro and Belém, Catarina and Soares, Carlos and Bizarro, Pedro},
  booktitle={2021 IEEE International Conference on Data Mining (ICDM)}, 
  title={Promoting Fairness through Hyperparameter Optimization}, 
  year={2021},
  volume={},
  number={},
  pages={1036-1041},
  doi={10.1109/ICDM51629.2021.00119}}


@article{Pedreschi2008,
   abstract = {In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on member- ship to a category or a minority, without regard to individ- ual merit. Rules extracted from databases by data mining techniques, such as classification or association rules, when used for decision tasks such as benefit or credit approval, can be discriminatory in the above sense. In this paper, the notion of discriminatory classification rules is introduced and studied. Providing a guarantee of non-discrimination is shown to be a non trivial task. A na¨ıve approach, like tak- ing away all discriminatory attributes, is shown to be not enough when other background knowledge is available. Our approach leads to a precise formulation of the redlining prob- lem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowl- edge. An empirical assessment of the results on the German credit dataset is also provided},
   author = {Dino Pedreschi and Salvatore Ruggieri and Franco Turini},
   doi = {10.1145/1401890.1401959},
   isbn = {9781605581934},
   issn = {0309-0167 (Print)},
   journal = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08},
   keywords = {classification rules,discrimination},
   pages = {560},
   pmid = {7698734},
   title = {Discrimination-aware data mining},
   url = {http://dl.acm.org/citation.cfm?doid=1401890.1401959},
   year = {2008},
}

@article{Kamiran2009,
   abstract = {Classification models usually make predictions on the basis of training data. If the training data is biased towards certain groups or classes of objects, e.g., there is racial discrimination towards black people, the learned model will also show discriminatory behavior towards that particular community. This partial attitude of the learned model may lead to biased outcomes when labeling future unlabeled data objects. Often, however, impartial classification results are desired or even required by law for future data objects in spite of having biased training data. In this paper, we tackle this problem by introducing a new classification scheme for learning unbiased models on biased training data. Our method is based on massaging the dataset by making the least intrusive modifications which lead to an unbiased dataset. On this modified dataset we then learn a non-discriminating classifier. The proposed method has been implemented and experimental results on a credit approval dataset show promising results: in all experiments our method is able to reduce the prejudicial behavior for future classification significantly without loosing too much predictive accuracy.},
   author = {Faisal Kamiran and Toon Calders},
   doi = {10.1109/IC4.2009.4909197},
   isbn = {9781424433148},
   journal = {2009 2nd International Conference on Computer, Control and Communication, IC4 2009},
   title = {Classifying without discriminating},
   year = {2009},
}

@article{Kamiran2012,
   abstract = {Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e. g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data. © 2011 The Author(s).},
   author = {Faisal Kamiran and Toon Calders},
   doi = {10.1007/s10115-011-0463-8},
   issn = {02193116},
   issue = {1},
   journal = {Knowledge and Information Systems},
   keywords = {Classification,Discrimination-aware data mining,Preprocessing},
   pages = {1-33},
   publisher = {Springer London},
   title = {Data preprocessing techniques for classification without discrimination},
   volume = {33},
   year = {2012},
}


@article{Mercier2018,
   abstract = {In this article, we propose a new method for multiobjective optimization problems in which the objective functions are expressed as expectations of random functions. The present method is based on an extension of the classical stochastic gradient algorithm and a deterministic multiobjective algorithm, the Multiple Gradient Descent Algorithm (MGDA). In MGDA a descent direction common to all specified objective functions is identified through a result of convex geometry. The use of this common descent vector and the Pareto stationarity definition into the stochastic gradient algorithm makes the algorithm able to solve multiobjective problems. The mean square and almost sure convergence of this new algorithm are proven considering the classical stochastic gradient algorithm hypothesis. The algorithm efficiency is illustrated on a set of benchmarks with diverse complexity and assessed in comparison with two classical algorithms (NSGA-II, DMS) coupled with a Monte Carlo expectation estimator.},
   author = {Quentin Mercier and Fabrice Poirion and Jean Antoine Désidéri},
   doi = {10.1016/j.ejor.2018.05.064},
   issn = {03772217},
   issue = {3},
   journal = {European Journal of Operational Research},
   keywords = {Multiobjective stochastic optimization,Multiple gradient descent algorithm Common descent vector,Multiple objective programming,Stochastic gradient algorithm},
   month = {12},
   pages = {808-817},
   publisher = {Elsevier B.V.},
   title = {A stochastic multiple gradient descent algorithm},
   volume = {271},
   year = {2018},
}

@article{pareto1906manuale,
  title={Manuale di economica politica, societa editrice libraria},
  author={Pareto, Vilfredo},
  journal={Manual of political economy},
  volume={1971},
  year={1906}
}

@article{GIAGKIOZIS2015338,
abstract = {Decomposition-based methods are often cited as the solution to multi-objective nonconvex optimization problems with an increased number of objectives. These methods employ a scalarizing function to reduce the multi-objective problem into a set of single objective problems, which upon solution yield a good approximation of the set of optimal solutions. This set is commonly referred to as Pareto front. In this work we explore the implications of using decomposition-based methods over Pareto-based methods on algorithm convergence from a probabilistic point of view. Namely, we investigate whether there is an advantage of using a decomposition-based method, for example using the Chebyshev scalarizing function, over Pareto-based methods. We find that, under mild conditions on the objective function, the Chebyshev scalarizing function has an almost identical effect to Pareto-dominance relations when we consider the probability of finding superior solutions for algorithms that follow a balanced trajectory. We propose the hypothesis that this seemingly contradicting result compared with currently available empirical evidence, signals that the disparity in performance between Pareto-based and decomposition-based methods is due to the inability of the former class of algorithms to follow a balanced trajectory. We also link generalized decomposition to the results in this work and show how to obtain optimal scalarizing functions for a given problem, subject to prior assumptions on the Pareto front geometry.},
author = {Giagkiozis, I and Fleming, P J},
doi = {https://doi.org/10.1016/j.ins.2014.08.071},
issn = {0020-0255},
journal = {Information Sciences},
keywords = { Chebyshev decomposition, Decomposition-based methods, Pareto-based methods,Multi-objective optimization},
pages = {338--350},
title = {{Methods for multi-objective optimization: An analysis}},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514009074},
volume = {293},
year = {2015}
}


@ARTICLE{Fortunato2010,
  author  = {Fortunato, S.},
  title   = {Community detection in graphs},
  journal = {Phys. Rep.-Rev. Sec. Phys. Lett.}, 
  volume  = {486},
  year    = {2010},
  pages   = {75-174}
}

@ARTICLE{NewmanGirvan2004,
  author  = {Newman, M. E. J. and Girvan, M.},
  title   = {Finding and evaluating community structure in networks},
  journal = {Phys. Rev. E.}, 
  volume  = {69},
  year    = {2004},
  pages   = {026113}
}

@ARTICLE{Vehlowetal2013,
  author  = {Vehlow, C. and Reinhardt, T. and Weiskopf, D.},
  title   = {Visualizing Fuzzy Overlapping Communities in Networks},
  journal = {IEEE Trans. Vis. Comput. Graph.}, 
  volume  = {19},
  year    = {2013},
  pages   = {2486-2495}
}

@ARTICLE{Raghavanetal2007,
  author  = {Raghavan, U. and Albert, R. and Kumara, S.},
  title   = {Near linear time algorithm to detect community structures in large-scale networks},
  journal = {Phys. Rev E.}, 
  volume  = {76},
  year    = {2007},
  pages   = {036106}
}

@ARTICLE{SubeljBajec2011a,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Robust network community detection using balanced propagation},
  journal = {Eur. Phys. J. B.}, 
  volume  = {81},
  year    = {2011},
  pages   = {353-362}
}

@ARTICLE{Louetal2013,
  author  = {Lou, H. and Li, S. and Zhao, Y.},
  title   = {Detecting community structure using label propagation with weighted coherent neighborhood propinquity},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {3095-3105}
}

@ARTICLE{Clausetetal2004,
  author  = {Clauset, A. and Newman, M. E. J. and Moore, C.},
  title   = {Finding community structure in very large networks},
  journal = {Phys. Rev. E.}, 
  volume  = {70},
  year    = {2004},
  pages   = {066111}
}

@ARTICLE{Blondeletal2008,
  author  = {Blondel, V. D. and Guillaume, J. L. and Lambiotte, R. and Lefebvre, E.},
  title   = {Fast unfolding of communities in large networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {2008},
  year    = {2008},
  pages   = {P10008}
}

@ARTICLE{SobolevskyCampari2014,
  author  = {Sobolevsky, S. and Campari, R.},
  title   = {General optimization technique for high-quality community detection in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {90},
  year    = {2014},
  pages   = {012811}
}

@ARTICLE{FortunatoBarthelemy2007,
  author  = {Fortunato, S. and Barthelemy, M.},
  title   = {Resolution limit in community detection},
  journal = {Proc. Natl. Acad. Sci. U. S. A.}, 
  volume  = {104},
  year    = {2007},
  pages   = {36-41}
}

@ARTICLE{SubeljBajec2011b,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Unfolding communities in large complex networks: Combining defensive and offensive label propagation for core extraction},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {036103}
}

@ARTICLE{WangLi2013,
  author  = {Wang, X. and Li, J.},
  title   = {Detecting communities by the core-vertex and intimate degree in complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {2555-2563}
}

@ARTICLE{Lietal2013,
  author  = {Li, J. and Wang, X. and Eustace, J.},
  title   = {Detecting overlapping communities by seed community in weighted complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6125-6134}
}

@ARTICLE{Fabioetal2013,
  author  = {Fabio, D. R. and Fabio, D. and Carlo, P.},
  title   = {Profiling core-periphery network structure by random walkers},
  journal = {Sci. Rep.}, 
  volume  = {3},
  year    = {2013},
  pages   = {1467}
}

@ARTICLE{Chenetal2013,
  author  = {Chen, Q. and Wu, T. T. and Fang, M.},
  title   = {Detecting local community structure in complex networks based on local degree central nodes},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {529-537}
}

@ARTICLE{Zhangetal2007,
  author  = {Zhang, S. and Wang, R. and Zhang, X.},
  title   = {Identification of overlapping community structure in complex networks using fuzzy c-means clustering},
  journal = {Physica A.}, 
  volume  = {374},
  year    = {2007},
  pages   = {483-490}
}

@ARTICLE{Nepuszetal2008,
  author  = {Nepusz, T. and Petr\'oczi, A. and N\'egyessy, L. and Bazs\'o, F.},
  title   = {Fuzzy communities and the concept of bridgeness in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {77},
  year    = {2008},
  pages   = {016107}
}

@ARTICLE{FabricioLiang2013,
  author  = {Fabricio, B. and Liang, Z.},
  title   = {Fuzzy community structure detection by particle competition and cooperation},
  journal = {Soft Comput.}, 
  volume  = {17},
  year    = {2013},
  pages   = {659-673}
}

@ARTICLE{Sunetal2011,
  author  = {Sun, P. and Gao, L. and Han, S.},
  title   = {Identification of overlapping and non-overlapping community structure by fuzzy clustering in complex networks},
  journal = {Inf. Sci.}, 
  volume  = {181},
  year    = {2011},
  pages   = {1060-1071}
}

@ARTICLE{Wangetal2013,
  author  = {Wang, W. and Liu, D. and Liu, X. and Pan, L.},
  title   = {Fuzzy overlapping community detection based on local random walk and multidimensional scaling},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6578-6586}
}

@ARTICLE{Psorakisetal2011,
  author  = {Psorakis, I. and Roberts, S. and Ebden, M. and Sheldon, B.},
  title   = {Overlapping community detection using Bayesian non-negative matrix factorization},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {066114}
}

@CONFERENCE{ZhangYeung2012,
  author  = {Zhang, Y. and Yeung, D.},
  title   = {Overlapping Community Detection via Bounded Nonnegative Matrix Tri-Factorization},
  booktitle = {In Proc. ACM SIGKDD Conf.}, 
  year    = {2012},
  pages   = {606-614}
}

@ARTICLE{Liu2010,
  author  = {Liu, J.},
  title   = {Fuzzy modularity and fuzzy community structure in networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {77},
  year    = {2010},
  pages   = {547-557}
}

@ARTICLE{Havensetal2013,
  author  = {Havens, T. C. and Bezdek, J. C. and Leckie, C., Ramamohanarao, K. and Palaniswami, M.},
  title   = {A Soft Modularity Function For Detecting Fuzzy Communities in Social Networks},
  journal = {IEEE Trans. Fuzzy Syst.}, 
  volume  = {21},
  year    = {2013},
  pages   = {1170-1175}
}

@misc{Newman2013,
  author = {Newman, M. E. J.},
  title  = {Network data},
  howpublished = "\url{http://www-personal.umich.edu/~mejn/netdata/}",
  year = {2013}
}

@ARTICLE{SubeljBajec2012,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Ubiquitousness of link-density and link-pattern communities in real-world networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {85},
  year    = {2012},
  pages   = {1-11}
}

@ARTICLE{Lancichinettietal2008,
  author  = {Lancichinetti, A. and Fortunato, S. and Radicchi, F.},
  title   = {Benchmark graphs for testing community detection algorithms},
  journal = {Phys. Rev. E.}, 
  volume  = {78},
  year    = {2008},
  pages   = {046110}
}

@ARTICLE{Liuetal2014,
  author  = {Liu, W. and Pellegrini, M. and Wang, X.},
  title   = {Detecting Communities Based on Network Topology},
  journal = {Sci. Rep.}, 
  volume  = {4},
  year    = {2014},
  pages   = {5739}
}

@ARTICLE{Danonetal2005,
  author  = {Danon, L. and Diaz-Guilera, A. and Duch, J. and Arenas, A.},
  title   = {Comparing community structure identification},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2005},
  pages   = {P09008}
}

@ARTICLE{Gregory2011,
  author  = {Gregory, S.},
  title   = {Fuzzy overlapping communities in networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2011},
  pages   = {P02017}
}

@ARTICLE{LancichinettiFortunato2009,
  author  = {Lancichinetti, A. and Fortunato, S.},
  title   = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
  journal = {Phys. Rev. E.}, 
  volume  = {80},
  year    = {2009},
  pages   = {016118}
}

@CONFERENCE{HullermeierRifqi2009,
  author  = {Hullermeier, E. and Rifqi, M.},
  title   = {A Fuzzy Variant of the Rand Index for Comparing Clustering Structures},
  booktitle = {in Proc. IFSA/EUSFLAT Conf.}, 
  year    = {2009},
  pages   = {1294-1298}
}


@article{Simpson,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2284382},
 abstract = {This paradox is the possibility of $P(A \mid B) < P(A \mid B\prime)$ even though P(A ∣ B) ≥ P(A ∣ B′) both under the additional condition C and under the complement C′ of that condition. Details are given on why this can happen and how extreme the inequalities can be. An example shows that Savage's sure-thing principle ("If you would definitely prefer g to f, either knowing that the event C obtained, or knowing that C did not obtain, then you definitely prefer g to f.") is not applicable to alternatives f and g that involve sequential operations.},
 author = {Colin R. Blyth},
 journal = {Journal of the American Statistical Association},
 number = {338},
 pages = {364--366},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {On Simpson's Paradox and the Sure-Thing Principle},
 urldate = {2024-04-22},
 volume = {67},
 year = {1972}
}