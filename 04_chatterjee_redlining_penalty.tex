\chapter{Chatterjee Redlining Penalty}

\section{Preliminaries}

The Pearson correlation coefficient, denoted as $\rho_{XY}$, is a measure of the linear relationship between two variables $X$ and $Y$. It is defined as:
\[
\rho_{XY} = \frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y}
\]
where $\mathrm{Cov}(X, Y)$ is the covariance of $X$ and $Y$, and $\sigma_X$ and $\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively. The Pearson correlation coefficient ranges from -1 to 1, where 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.

Spearman's rank correlation coefficient, denoted as $\rho_s$, measures the strength and direction of the monotonic relationship between two ranked variables. It is defined as the Pearson correlation coefficient between the ranked variables. If $r_i$ and $s_i$ are the ranks of $X_i$ and $Y_i$ respectively, then:
\[
\rho_s = \frac{\mathrm{Cov}(r, s)}{\sigma_r \sigma_s}
\]
where $\mathrm{Cov}(r, s)$ is the covariance of the rank variables, and $\sigma_r$ and $\sigma_s$ are the standard deviations of the ranks. The coefficient also ranges from -1 to 1.

Kendall's tau, denoted as $\tau$, is a measure of the ordinal association between two variables. It is defined as:
\[
\tau = \frac{C - D}{\binom{n}{2}}
\]
where $C$ is the number of concordant pairs, $D$ is the number of discordant pairs, and $\binom{n}{2}$ is the total number of pairs. A pair of observations $(X_i, Y_i)$ and $(X_j, Y_j)$ is concordant if the ranks for both elements agree, i.e., either $X_i > X_j$ and $Y_i > Y_j$, or $X_i < X_j$ and $Y_i < Y_j$. It ranges from -1 to 1.

Chatterjee's correlation coefficient, denoted as $\xi$, is designed to measure the degree of dependence between two variables without assuming any specific type of relationship. Given a dataset $(X, Y)$ with $n$ pairs, the coefficient is defined as:
\[
\xi_n(X,Y) := 1 - \frac{3 \sum_{i=1}^{n-1} |r_{i+1} - r_i|}{n^2 - 1}
\]
where $r_i$ is the rank of $Y_i$ in the ordered sequence of $Y$ values corresponding to the sorted $X$ values. This coefficient ranges from 0 to 1, where 0 indicates independence and 1 indicates a perfect functional relationship. For the general case with ties, a more complex formula involving additional terms to handle the ties is used.


\section{Proposal}

The Chatterjee Redlining Penalty is defined as a regularization term that penalizes the weights associated with features that are highly correlated with the sensitive attribute. This penalty term is incorporated into the loss function of the neural network in order to produce fairer predictions. Consider a dataset \(X \in \mathbb{R}^{n \times d}\) where \(n\) represents the number of instances and \(d\) represents the number of features. Let \(X_i \in \mathbb{R}^n\) denote the \(i\)-th feature of the dataset, and let \(A = X_i \in \mathbb{R}^n\) be a sensitive (protected) feature for some \(i\). In this neural network, \(\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}\) is the weight matrix for the first hidden layer, with \(h\) being the number of neurons in this layer. Additionally, \(\lambda \in \mathbb{R}^d\) is a vector representing the regularization strengths for each feature, and $\lambda$ is a scalar that controls the overall strength of the regularization.



The regularization term \(R(\mathbf{W}^{(1)})\) applied to the weight matrix \(\mathbf{W}^{(1)}\) of the first hidden layer is defined in Equation~\ref{eq:xi_reg}

\begin{equation}\label{eq:xi_reg}
R(\mathbf{W}^{(1)}) = \sum_{i=1}^d \xi_n(X_i,\,A) \sum_{j=1}^h (W^{(1)}_{ij})^2,
\end{equation}
where,the Chatterjee's Xi Correlation Coefficient  $\xi_n(X_i,\,A)$  between the $i$-th input feature $X_i$ and the sensitive feature $A$ acts as the regularization strength for the $i$-th input feature. Here $W^{(1)}_{ij}$ are the weights connecting the $i$-th input feature to the $j$-th neuron in the first hidden layer. The greater $i$-th input feature dependence on sensitive feature the greater the penalization factor enforcing lower values to those weights

The total loss function \(L\) for the multilayer perceptron (MLP), incorporating the sensitive-feature-specific \(L_2\) regularization, is given by Equation~\ref{eq:total_regularized_loss}
\begin{equation}\label{eq:total_regularized_loss}
L = L_0 + \lambda \; R(\mathbf{W}^{(1)}),
\end{equation}
where $L_0$ is the primary loss function of the network. This formulation ensures that the model's learning process penalizes the weights associated with features highly correlated with the sensitive attribute, thereby reducing the potential for biased decisions.


\section{Experimental setup}

\section{Results and discussion}
