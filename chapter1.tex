\chapter{Introduction}

The issue of fairness in machine learning has recently risen to prominence due to its implications in real-world decision-making systems~\citep{Mehrabi2019,Hutchinson2019}. Addressing biases and discrimination is a relevant frontier in decision-making systems, as equitable outcomes across various demographic groups is both an ethical imperative and often a legal requirement. Though fairness is a multifaceted concept, it has been deeply examined within the context of machine learning. The literature presents a variety of fairness definitions, drawing concepts from political philosophy and computational techniques~\citep{caton2023, Hutchinson2019}. Choosing an equitable machine learning model requires the selection of a fitting definition of fairness, tailored to the specific problem at hand. Many such definitions can be precisely articulated, allowing models to be evaluated based on their predictions.

One inherent challenge in fair machine learning is the balance between fairness and accuracy. Efforts to mitigate unfairness often compromise the model's predictive performance, a trade-off that has been well documented~\citep{Mehrabi2019, caton2023}. Predictors that are less biased against marginalized groups may deviate from the true class, resulting in sub-optimal performance. Also, introducing fairness considerations adds constraints to the model, further complicating the optimization process~\citep{Zafar2017b}. 

In light of these challenges, we introduce the Fair Transition Loss, a novel approach to fair classification. This method estimates the influence of historical and societal biases on outcome probabilities for distinct groups within dataset. For instance, individuals from marginalized groups might have lower chances of favorable outcomes compared to their counterparts from privileged groups. Such disparate probabilities can be represented by transition matrices. Drawing inspiration from label noise robustness, we incorporate these transition matrices information into the loss function to promote fairness. The proposed method has some hyperarameters, chosen by a Multi-Objective Optimization approach combining both fairness and model performance with a linear smooth objective. This objective is defined in such a way that it is possible to use this approach to optimize a variety of fairness and performance metrics.

The primary contribution of this study is the conceptualization of the Fair Transition Loss, a novel loss function influenced by label noise methodologies. In benchmark tests across common fair classification tasks, our empirical results demonstrate that this method consistently outperforms many leading in-processing fair classification techniques in a variety of scenarios. To the best of our knowledge, this work is the first of its kind to apply label noise techniques directly within the model to mitigate unfairness. 

%The remainder of this paper is structured as follows: Section~\ref{sec:related} delineates related works encompassing fairness definitions, metrics and methods, multi-objective optimization in fair machine learning, and classifications in the presence of label noise. We also spotlight studies bridging fairness and label noise. In Section~\ref{sec:proposal}, we describe the Fair Transition Loss and its underlying principles. Section~\ref{sec:experimental} details our experimental methodology and Section~\ref{sec:results} discuss results for common fair classification tasks. In Section~\ref{sec:conclusions} we present conclusions drawn from our study and insights to research directions.
